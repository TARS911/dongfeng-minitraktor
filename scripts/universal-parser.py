#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è e-commerce —Å–∞–π—Ç–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
"""

import requests
from bs4 import BeautifulSoup
import csv
import json
import time
import sys
import re
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional

class UniversalParser:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        })
        self.products = []

    def fetch_page(self, url: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(3):
            try:
                print(f"Fetching: {url} (attempt {attempt + 1}/3)")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except Exception as e:
                print(f"Error: {e}")
                if attempt < 2:
                    time.sleep(5)
        return None

    def find_product_containers(self, soup: BeautifulSoup) -> List:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å —Ç–æ–≤–∞—Ä–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
        strategies = [
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º product
            lambda: soup.find_all(['div', 'article', 'li'], class_=re.compile(r'product(?!-card__)', re.I)),

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–∏—Å–∫ –ø–æ data-–∞—Ç—Ä–∏–±—É—Ç–∞–º
            lambda: soup.find_all(attrs={'data-product-id': True}),
            lambda: soup.find_all(attrs={'data-id': True}),

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ itemtype –¥–ª—è Schema.org
            lambda: soup.find_all(attrs={'itemtype': re.compile(r'Product', re.I)}),

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ (—Ä–æ–¥–∏—Ç–µ–ª—å —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ç–æ–≤–∞—Ä–∞–º–∏)
            lambda: soup.find_all(['div', 'ul'], class_=re.compile(r'catalog|products|items|grid|list', re.I)),

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ü–æ–∏—Å–∫ article —Ç–µ–≥–æ–≤
            lambda: soup.find_all('article'),

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 6: –ü–æ–∏—Å–∫ –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ —Ç–æ–≤–∞—Ä—ã
            lambda: [a.parent for a in soup.find_all('a', href=re.compile(r'/product|/item|/p/', re.I))],
        ]

        for i, strategy in enumerate(strategies):
            try:
                containers = strategy()
                if containers:
                    print(f"Strategy {i+1} found {len(containers)} containers")
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞—à–ª–∏ –Ω–µ –≤–µ—Å—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∫–∞—Ç–∞–ª–æ–≥–∞, –∞ –∏–º–µ–Ω–Ω–æ —Ç–æ–≤–∞—Ä—ã
                    if 2 < len(containers) < 300:
                        return containers
            except Exception as e:
                print(f"Strategy {i+1} failed: {e}")
                continue

        return []

    def extract_text(self, elem, selectors: List) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞, –ø–µ—Ä–µ–±–∏—Ä–∞—è —Å–µ–ª–µ–∫—Ç–æ—Ä—ã"""
        if not elem:
            return ""

        for selector in selectors:
            try:
                if isinstance(selector, str):
                    # CSS —Å–µ–ª–µ–∫—Ç–æ—Ä
                    found = elem.select_one(selector)
                    if found:
                        return found.get_text(strip=True)
                elif callable(selector):
                    # –§—É–Ω–∫—Ü–∏—è-—Å–µ–ª–µ–∫—Ç–æ—Ä
                    result = selector(elem)
                    if result:
                        return result if isinstance(result, str) else result.get_text(strip=True)
            except:
                continue

        return ""

    def extract_price(self, elem) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        selectors = [
            '[itemprop="price"]',
            '.price',
            '[class*="price"]',
            '[data-price]',
            lambda e: e.find(string=re.compile(r'\d+\s*‚ÇΩ|\d+\s*—Ä—É–±', re.I)),
        ]

        price_text = self.extract_text(elem, selectors)

        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º data-price –∞—Ç—Ä–∏–±—É—Ç
        if not price_text:
            price_elem = elem.find(attrs={'data-price': True})
            if price_elem:
                price_text = price_elem.get('data-price', '')

        # –û—á–∏—â–∞–µ–º —Ü–µ–Ω—É
        if price_text:
            price_text = re.sub(r'[^\d.,]', '', price_text)
            price_text = price_text.replace(',', '.')

        return price_text

    def extract_title(self, elem) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞"""
        selectors = [
            '[itemprop="name"]',
            'h1', 'h2', 'h3', 'h4',
            '.title', '[class*="title"]',
            '.name', '[class*="name"]',
            'a[href*="/product"]',
            'a[href*="/item"]',
        ]
        return self.extract_text(elem, selectors)

    def extract_article(self, elem) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—Ä—Ç–∏–∫—É–ª"""
        selectors = [
            '[itemprop="sku"]',
            '.article', '.sku', '.code',
            '[class*="article"]', '[class*="sku"]',
            lambda e: e.find(string=re.compile(r'–∞—Ä—Ç–∏–∫—É–ª|sku|–∫–æ–¥', re.I)),
        ]

        article = self.extract_text(elem, selectors)

        # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤
        if article:
            article = re.sub(r'–∞—Ä—Ç–∏–∫—É–ª:?\s*|sku:?\s*|–∫–æ–¥:?\s*', '', article, flags=re.I)

        return article.strip()

    def extract_image(self, elem) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∫–∞—Ä—Ç–∏–Ω–∫–∏"""
        img = elem.find('img')
        if img:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            for attr in ['data-src', 'src', 'data-lazy-src']:
                url = img.get(attr)
                if url:
                    return urljoin(self.base_url, url)
        return ""

    def extract_url(self, elem) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL —Ç–æ–≤–∞—Ä–∞"""
        link = elem.find('a', href=True)
        if link:
            return urljoin(self.base_url, link['href'])
        return ""

    def parse_product(self, container) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        try:
            product = {
                'title': self.extract_title(container),
                'article': self.extract_article(container),
                'price': self.extract_price(container),
                'url': self.extract_url(container),
                'image_url': self.extract_image(container),
                'stock': '–í –Ω–∞–ª–∏—á–∏–∏',  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                'description': ''
            }

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ
            stock_text = container.get_text()
            if '–Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏' in stock_text.lower() or '–ø–æ–¥ –∑–∞–∫–∞–∑' in stock_text.lower():
                product['stock'] = '–ü–æ–¥ –∑–∞–∫–∞–∑'

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ç–æ–≤–∞—Ä—ã
            if not product['title'] or len(product['title']) < 3:
                return None

            return product

        except Exception as e:
            print(f"Error parsing product: {e}")
            return None

    def parse_page(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞"""
        html = self.fetch_page(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        with open('/tmp/zip-agro-debug.html', 'w', encoding='utf-8') as f:
            f.write(soup.prettify()[:50000])  # –ü–µ—Ä–≤—ã–µ 50KB –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        containers = self.find_product_containers(soup)
        products = []

        print(f"Processing {len(containers)} product containers...")

        for container in containers:
            product = self.parse_product(container)
            if product:
                products.append(product)
                if len(products) % 10 == 0:
                    print(f"Parsed {len(products)} products...")

        return products

    def parse_catalog(self, start_url: str, total_pages: int = 5, limit: int = 100) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞"""
        all_products = []

        for page_num in range(1, total_pages + 1):
            url = f"{start_url}?limit={limit}&page={page_num}"
            products = self.parse_page(url)

            print(f"Page {page_num}: found {len(products)} products")
            all_products.extend(products)

            if page_num < total_pages:
                time.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏

        return all_products

    def save_to_csv(self, filename: str = 'parsed_data/zip-agro-dongfeng.csv'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–≤–∞—Ä—ã –≤ CSV"""
        if not self.products:
            print("No products to save!")
            return

        import os
        os.makedirs(os.path.dirname(filename), exist_ok=True)

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            if self.products:
                writer = csv.DictWriter(f, fieldnames=self.products[0].keys())
                writer.writeheader()
                writer.writerows(self.products)

        print(f"\n‚úì Saved {len(self.products)} products to {filename}")

        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        json_file = filename.replace('.csv', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.products, f, ensure_ascii=False, indent=2)
        print(f"‚úì Saved JSON to {json_file}")


def main():
    print("="  * 70)
    print("–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–ê–†–°–ï–† ZIP-AGRO.RU (DongFeng)")
    print("=" * 70)

    parser = UniversalParser("https://zip-agro.ru")

    # –ü–∞—Ä—Å–∏–º –≤—Å–µ 5 —Å—Ç—Ä–∞–Ω–∏—Ü
    products = parser.parse_catalog(
        start_url="https://zip-agro.ru/dongfeng",
        total_pages=5,
        limit=100
    )

    parser.products = products

    print(f"\n{'=' * 70}")
    print(f"–í–°–ï–ì–û –°–ü–ê–†–°–ï–ù–û: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
    print(f"{'=' * 70}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    parser.save_to_csv()

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
    if products:
        print("\nüì¶ –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–≤–∞—Ä–æ–≤:")
        for i, p in enumerate(products[:5], 1):
            print(f"\n{i}. {p['title']}")
            print(f"   –ê—Ä—Ç–∏–∫—É–ª: {p['article']}")
            print(f"   –¶–µ–Ω–∞: {p['price']} ‚ÇΩ")
            print(f"   URL: {p['url']}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö† –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
