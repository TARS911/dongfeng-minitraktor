#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ "–ó–∞–ø—á–∞—Å—Ç–∏ –¥–ª—è —Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤"
"""

import json
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://xn----7sbabpgpk4bsbesjp1f.xn--p1ai"
CATEGORY_URL = f"{BASE_URL}/product-category/–∑–∞–ø—á–∞—Å—Ç–∏-–¥–ª—è-—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤/"

session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
})

def parse_subcategories():
    """–ü–∞—Ä—Å–∏—Ç –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    print("üîç –ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑: –ó–∞–ø—á–∞—Å—Ç–∏ –¥–ª—è —Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤")
    print(f"üìç URL: {CATEGORY_URL}\n")

    try:
        response = session.get(CATEGORY_URL, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        subcategories = []

        # –ú–µ—Ç–æ–¥ 1: –ò—â–µ–º —á–µ—Ä–µ–∑ .product-categories
        subcat_links = soup.select(".product-categories a")
        print(f"‚úì –ú–µ—Ç–æ–¥ 1 (.product-categories a): –Ω–∞–π–¥–µ–Ω–æ {len(subcat_links)} —Å—Å—ã–ª–æ–∫")

        for link in subcat_links:
            href = link.get("href")
            name = link.get_text(strip=True)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
            count = 0
            count_elem = link.select_one(".count")
            if count_elem:
                count_text = count_elem.get_text(strip=True)
                count = int(count_text.replace("(", "").replace(")", ""))

            if href and name:
                subcategories.append({
                    "name": name,
                    "url": href,
                    "count": count
                })

        # –ú–µ—Ç–æ–¥ 2: –ò—â–µ–º —á–µ—Ä–µ–∑ .cat-item
        if not subcategories:
            cat_items = soup.select(".cat-item a, .product-category a")
            print(f"‚úì –ú–µ—Ç–æ–¥ 2 (.cat-item a): –Ω–∞–π–¥–µ–Ω–æ {len(cat_items)} —Å—Å—ã–ª–æ–∫")

            for link in cat_items:
                href = link.get("href")
                name = link.get_text(strip=True)

                if href and name:
                    subcategories.append({
                        "name": name,
                        "url": href,
                        "count": 0
                    })

        # –ú–µ—Ç–æ–¥ 3: –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å product-category –≤ URL
        if not subcategories:
            all_links = soup.select("a[href*='product-category']")
            print(f"‚úì –ú–µ—Ç–æ–¥ 3 (–≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å product-category): –Ω–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫")

            seen = set()
            for link in all_links:
                href = link.get("href")
                name = link.get_text(strip=True)

                if href and name and href not in seen:
                    seen.add(href)
                    subcategories.append({
                        "name": name,
                        "url": href,
                        "count": 0
                    })

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL
        unique_subcats = {}
        for subcat in subcategories:
            url = subcat["url"]
            if url not in unique_subcats:
                unique_subcats[url] = subcat

        result = list(unique_subcats.values())

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–≤–∞—Ä–æ–≤
        result.sort(key=lambda x: x.get("count", 0), reverse=True)

        return result

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return []

def main():
    print("=" * 70)
    print("  –ü–ê–†–°–ò–ù–ì –ü–û–î–ö–ê–¢–ï–ì–û–†–ò–ô: –ó–∞–ø—á–∞—Å—Ç–∏ –¥–ª—è —Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤")
    print("=" * 70 + "\n")

    subcategories = parse_subcategories()

    if subcategories:
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(subcategories)}\n")
        print("=" * 70)

        total_products = 0
        for i, subcat in enumerate(subcategories, 1):
            count = subcat.get("count", 0)
            total_products += count

            print(f"{i:2}. {subcat['name']}")
            print(f"    URL: {subcat['url']}")
            if count > 0:
                print(f"    –¢–æ–≤–∞—Ä–æ–≤: {count}")
            print()

        print("=" * 70)
        if total_products > 0:
            print(f"üìä –í–°–ï–ì–û —Ç–æ–≤–∞—Ä–æ–≤ –≤ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö: {total_products}")
        print("=" * 70)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        output_file = "parsed_data/agrodom/tractor-parts-subcategories.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(subcategories, f, ensure_ascii=False, indent=2)

        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")

    else:
        print("\n‚ùå –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print("\nüîç –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        response = session.get(CATEGORY_URL, timeout=30)
        soup = BeautifulSoup(response.content, "html.parser")

        print("\nüìã –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ:")
        all_classes = set()
        for elem in soup.find_all(class_=True):
            all_classes.update(elem.get("class", []))

        relevant_classes = [c for c in all_classes if any(
            word in c.lower() for word in ['category', 'product', 'cat', 'menu', 'nav']
        )]

        for cls in sorted(relevant_classes)[:20]:
            print(f"  - .{cls}")

if __name__ == "__main__":
    main()
