#!/usr/bin/env python3
"""
–¢–æ—á–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è ZIP-AGRO.RU (DongFeng)
–í–µ—Ä—Å–∏—è 2 - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
"""

import requests
from bs4 import BeautifulSoup
import csv
import json
import time
import re
from urllib.parse import urljoin

class ZipAgroParser:
    def __init__(self):
        self.base_url = "https://zip-agro.ru"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'Connection': 'keep-alive'
        })
        self.products = []

    def fetch_page(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            print(f"üìÑ Fetching: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None

    def parse_product_item(self, item):
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω —Ç–æ–≤–∞—Ä –∏–∑ <div class="product-item">"""
        try:
            product = {}

            # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ (–≤ .product-name a)
            name_link = item.select_one('.product-name a')
            if name_link:
                product['title'] = name_link.get_text(strip=True)
                product['url'] = urljoin(self.base_url, name_link.get('href', ''))
            else:
                return None

            # –ö–∞—Ä—Ç–∏–Ω–∫–∞
            img = item.select_one('.product-image img')
            if img:
                img_src = img.get('data-src') or img.get('src')
                product['image_url'] = urljoin(self.base_url, img_src) if img_src else ''
            else:
                product['image_url'] = ''

            # –¶–µ–Ω–∞ (–≤ .price .h6)
            price_elem = item.select_one('.price .h6')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã (—É–±–∏—Ä–∞–µ–º "—Ä.")
                price_match = re.search(r'([\d\s,.]+)', price_text)
                if price_match:
                    price = price_match.group(1).replace(' ', '').replace(',', '.')
                    product['price'] = price
                else:
                    product['price'] = ''
            else:
                product['price'] = ''

            # –ê—Ä—Ç–∏–∫—É–ª (–≤ .badge.stiker-upc)
            code_elem = item.select_one('.badge.stiker-upc')
            if code_elem:
                product['article'] = code_elem.get_text(strip=True)
            else:
                product['article'] = ''

            # –ù–∞–ª–∏—á–∏–µ (–∏—â–µ–º .product-stock –∏–ª–∏ in-stock)
            stock_elem = item.select_one('[class*="stock"]')
            if stock_elem:
                stock_text = stock_elem.get_text(strip=True).lower()
                if '–Ω–µ—Ç' in stock_text or '–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç' in stock_text:
                    product['stock'] = '–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏'
                elif '–∑–∞–∫–∞–∑' in stock_text:
                    product['stock'] = '–ü–æ–¥ –∑–∞–∫–∞–∑'
                else:
                    product['stock'] = '–í –Ω–∞–ª–∏—á–∏–∏'
            else:
                product['stock'] = '–í –Ω–∞–ª–∏—á–∏–∏'

            # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–∏–∑ .product-text, –¥–∞–∂–µ –µ—Å–ª–∏ —Å–∫—Ä—ã—Ç–æ —á–µ—Ä–µ–∑ d-none)
            desc_elem = item.select_one('.product-text')
            product['description'] = desc_elem.get_text(strip=True) if desc_elem else ''

            # –ö–∞—Ç–µ–≥–æ—Ä–∏—è/–ë—Ä–µ–Ω–¥
            product['brand'] = 'DongFeng'
            product['category'] = '–ó–∞–ø—á–∞—Å—Ç–∏'

            return product

        except Exception as e:
            print(f"‚ö† Error parsing item: {e}")
            return None

    def parse_page(self, url):
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        html = self.fetch_page(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')

        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ç–æ–≤–∞—Ä–∞–º–∏: #content .products-container
        container = soup.find(id='content')
        if not container:
            print("‚ö† #content not found")
            return []

        # –ò—â–µ–º –≤—Å–µ .product-item
        items = container.find_all('div', class_='product-item')
        print(f"‚úì Found {len(items)} product items")

        products = []
        for i, item in enumerate(items, 1):
            product = self.parse_product_item(item)
            if product and product.get('title'):
                products.append(product)

                if i % 20 == 0:
                    print(f"  Parsed {i}/{len(items)}...")

        print(f"‚úì Successfully parsed {len(products)} products from this page")
        return products

    def parse_all_pages(self, start_url, total_pages=5, limit=100):
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        all_products = []

        for page_num in range(1, total_pages + 1):
            url = f"{start_url}?limit={limit}&page={page_num}"
            print(f"\n{'='*70}")
            print(f"PAGE {page_num}/{total_pages}")
            print(f"{'='*70}")

            products = self.parse_page(url)
            all_products.extend(products)

            print(f"üìä Total so far: {len(all_products)} products")

            if page_num < total_pages:
                print("‚è≥ Waiting 2 seconds...")
                time.sleep(2)

        return all_products

    def save_to_csv(self, products, filename='parsed_data/zip-agro-dongfeng.csv'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ CSV"""
        if not products:
            print("‚ùå No products to save!")
            return

        import os
        os.makedirs(os.path.dirname(filename), exist_ok=True)

        fieldnames = ['title', 'article', 'price', 'brand', 'category', 'stock',
                     'description', 'url', 'image_url']

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            writer.writerows(products)

        print(f"\n‚úÖ Saved {len(products)} products to {filename}")

        # JSON
        json_file = filename.replace('.csv', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Saved JSON to {json_file}")

    def show_stats(self, products):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print(f"\n{'='*70}")
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*70}")
        print(f"–í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")

        # –° –∞—Ä—Ç–∏–∫—É–ª–æ–º
        with_article = sum(1 for p in products if p.get('article'))
        print(f"–° –∞—Ä—Ç–∏–∫—É–ª–æ–º: {with_article}")

        # –° —Ü–µ–Ω–æ–π
        with_price = sum(1 for p in products if p.get('price'))
        print(f"–° —Ü–µ–Ω–æ–π: {with_price}")

        # –° –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏
        with_image = sum(1 for p in products if p.get('image_url'))
        print(f"–° –∫–∞—Ä—Ç–∏–Ω–∫–æ–π: {with_image}")

        print(f"\n{'='*70}")
        print("üì¶ –ü–†–ò–ú–ï–†–´ (–ø–µ—Ä–≤—ã–µ 5):")
        print(f"{'='*70}")

        for i, p in enumerate(products[:5], 1):
            print(f"\n{i}. {p['title']}")
            print(f"   –ê—Ä—Ç–∏–∫—É–ª: {p.get('article', '–ù–µ—Ç')}")
            print(f"   –¶–µ–Ω–∞: {p.get('price', '–ù–µ—Ç')} ‚ÇΩ")
            print(f"   URL: {p.get('url', '–ù–µ—Ç')[:80]}")


def main():
    print("=" * 70)
    print("ZIP-AGRO.RU DongFeng Parser v2")
    print("=" * 70)

    parser = ZipAgroParser()

    # –ü–∞—Ä—Å–∏–º 5 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ 100 —Ç–æ–≤–∞—Ä–æ–≤
    products = parser.parse_all_pages(
        start_url="https://zip-agro.ru/dongfeng",
        total_pages=5,
        limit=100
    )

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    parser.show_stats(products)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    parser.save_to_csv(products)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö† Interrupted")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
