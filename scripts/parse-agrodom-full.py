#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–û–õ–ù–´–ô –ü–ê–†–°–ï–† –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ —Å Agrodom
–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏—Ç –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
"""

import json
import os
import sys
import time

import requests
from bs4 import BeautifulSoup

sys.stdout.reconfigure(line_buffering=True)

BASE_URL = "https://xn----7sbabpgpk4bsbesjp1f.xn--p1ai"
OUTPUT_FILE = "parsed_data/agrodom/parts-full.json"

session = requests.Session()
session.headers.update(
    {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
)


def get_all_products_from_category(url, depth=0):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –µ—ë –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    indent = "  " * depth
    print(f"{indent}üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞: {url}")

    all_products = []
    seen_urls = set()

    try:
        response = session.get(url, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        # 1. –ò—â–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        subcats = soup.select(".product-categories a, .product-category a")
        if subcats and depth < 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É —Ä–µ–∫—É—Ä—Å–∏–∏
            print(f"{indent}  –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(subcats)}")
            for subcat in subcats:
                subcat_url = subcat.get("href")
                if subcat_url and subcat_url not in seen_urls:
                    seen_urls.add(subcat_url)
                    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–∞—Ä—Å–∏–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é
                    sub_products = get_all_products_from_category(subcat_url, depth + 1)
                    all_products.extend(sub_products)
                    time.sleep(0.3)

        # 2. –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        products = get_products_from_page(url)
        all_products.extend(products)

        # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
        pagination = soup.select(".page-numbers a")
        max_page = 1
        for link in pagination:
            text = link.get_text(strip=True)
            if text.isdigit():
                max_page = max(max_page, int(text))

        if max_page > 1:
            print(f"{indent}  –°—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {max_page}")
            for page in range(2, max_page + 1):
                page_url = f"{url}page/{page}/"
                page_products = get_products_from_page(page_url)
                all_products.extend(page_products)
                time.sleep(0.3)

        print(f"{indent}  ‚úÖ –¢–æ–≤–∞—Ä–æ–≤: {len(all_products)}")

    except Exception as e:
        print(f"{indent}  ‚ùå –û—à–∏–±–∫–∞: {e}")

    return all_products


def get_products_from_page(url):
    """–ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        response = session.get(url, timeout=20)
        soup = BeautifulSoup(response.content, "html.parser")
        products = []

        items = soup.select(".product, .type-product")

        for item in items:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—É –Ω–∏—Ö –µ—Å—Ç—å count –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏)
            name_elem = item.select_one(
                "h2, .product-title, .woocommerce-loop-product__title"
            )
            if not name_elem:
                continue

            name = name_elem.get_text(strip=True)

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —ç—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è (—Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–æ –≤ —Å–∫–æ–±–∫–∞—Ö)
            if "(" in name and ")" in name:
                continue

            link = item.select_one("a")
            price = item.select_one(".price .amount, .woocommerce-Price-amount")
            image = item.select_one("img")

            if name and link:
                products.append(
                    {
                        "name": name,
                        "price": price.get_text(strip=True) if price else "",
                        "image_url": image.get("src", "") if image else "",
                        "link": link.get("href", ""),
                    }
                )

        return products
    except:
        return []


def main():
    os.makedirs("parsed_data/agrodom", exist_ok=True)

    print("\n" + "=" * 70)
    print("–ü–û–õ–ù–´–ô –ü–ê–†–°–ò–ù–ì –í–°–ï–• –¢–û–í–ê–†–û–í AGRODOM")
    print("=" * 70 + "\n")

    # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–ø—á–∞—Å—Ç–µ–π
    main_categories = [
        f"{BASE_URL}/product-category/–¥–≤–∏–≥–∞—Ç–µ–ª—è-–¥–∏–∑–µ–ª—å–Ω—ã–µ/",
        f"{BASE_URL}/product-category/—Ñ–∏–ª—å—Ç—Ä–∞/",
        f"{BASE_URL}/product-category/–≥–∏–¥—Ä–∞–≤–ª–∏–∫–∞/",
        f"{BASE_URL}/product-category/–∫–∞—Ä–¥–∞–Ω–Ω—ã–µ-–≤–∞–ª—ã/",
        f"{BASE_URL}/product-category/—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ-–∫–æ–º–ø–ª–µ–∫—Ç—É—é—â–∏–µ/",
        f"{BASE_URL}/product-category/–∑–∏–ø/",
        f"{BASE_URL}/product-category/—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ-–∏–∑–¥–µ–ª–∏—è/",
        f"{BASE_URL}/product-category/–∑–∞–ø—á–∞—Å—Ç–∏-–¥–ª—è-—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤/",
        f"{BASE_URL}/product-category/–∑–∞–ø—á–∞—Å—Ç–∏-–¥–ª—è-–Ω–∞–≤–µ—Å–Ω–æ–≥–æ-–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è/",
        f"{BASE_URL}/product-category/–∑–∞–ø—á–∞—Å—Ç–∏-–¥–ª—è-–¥–∏–∑–µ–ª–µ–π/",
        f"{BASE_URL}/product-category/–∫–æ–ª—ë—Å–∞-—à–∏–Ω—ã-–≥—Ä—É–∑–∞/",
        f"{BASE_URL}/product-category/–ø—Ä–æ—á–∏–µ-–∑–∞–ø—á–∞—Å—Ç–∏/",
        f"{BASE_URL}/product-category/—Å—Ç–∞—Ä—Ç–µ—Ä—ã-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã/",
        f"{BASE_URL}/product-category/—Å–∏–¥–µ–Ω—å—è-–∫—Ä–µ—Å–ª–∞/",
    ]

    all_products = []
    seen_names = set()

    for i, cat_url in enumerate(main_categories, 1):
        print(f"\n[{i}/{len(main_categories)}]")

        try:
            products = get_all_products_from_category(cat_url)

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            for p in products:
                name_key = p["name"].lower().strip()
                if name_key not in seen_names:
                    seen_names.add(name_key)
                    all_products.append(p)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
                json.dump(all_products, f, ensure_ascii=False, indent=2)

            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ. –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(all_products)}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

    print("\n" + "=" * 70)
    print("‚úÖ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–Å–ù!")
    print("=" * 70)
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products)}")
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {OUTPUT_FILE}")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()
