#!/usr/bin/env python3
"""
ĞĞĞ”Ğ•Ğ–ĞĞ«Ğ™ ĞŸĞĞ Ğ¡Ğ•Ğ  AGRODOM Ñ BeautifulSoup + Requests
ĞŸĞ°Ñ€ÑĞ¸Ñ‚ Ğ’Ğ¡Ğ• Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹ ÑĞ¾ Ğ²ÑĞµÑ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹
"""

import json
import os
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ Ğ±ÑƒÑ„ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°
sys.stdout = os.fdopen(sys.stdout.fileno(), "w", buffering=1)
sys.stderr = os.fdopen(sys.stderr.fileno(), "w", buffering=1)

BASE_URL = "https://xn----7sbabpgpk4bsbesjp1f.xn--p1ai"
OUTPUT_FILE = "parsed_data/agrodom/parts-complete-bs4.json"
PROGRESS_FILE = "parsed_data/agrodom/parts-progress-bs4.json"

# ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚ĞµĞ¹
CATEGORIES = [
    {
        "name": "Ğ”Ğ²Ğ¸Ğ³Ğ°Ñ‚ĞµĞ»Ñ Ğ´Ğ¸Ğ·ĞµĞ»ÑŒĞ½Ñ‹Ğµ",
        "url": f"{BASE_URL}/product-category/Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ĞµĞ»Ñ-Ğ´Ğ¸Ğ·ĞµĞ»ÑŒĞ½Ñ‹Ğµ/",
    },
    {"name": "Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°", "url": f"{BASE_URL}/product-category/Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°/"},
    {"name": "Ğ“Ğ¸Ğ´Ñ€Ğ°Ğ²Ğ»Ğ¸ĞºĞ°", "url": f"{BASE_URL}/product-category/Ğ³Ğ¸Ğ´Ñ€Ğ°Ğ²Ğ»Ğ¸ĞºĞ°/"},
    {"name": "ĞšĞ°Ñ€Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ°Ğ»Ñ‹", "url": f"{BASE_URL}/product-category/ĞºĞ°Ñ€Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ-Ğ²Ğ°Ğ»Ñ‹/"},
    {
        "name": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑ‚ÑƒÑÑ‰Ğ¸Ğµ",
        "url": f"{BASE_URL}/product-category/ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ-ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑ‚ÑƒÑÑ‰Ğ¸Ğµ/",
    },
    {"name": "Ğ—Ğ˜ĞŸ", "url": f"{BASE_URL}/product-category/Ğ·Ğ¸Ğ¿/"},
    {
        "name": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ´ĞµĞ»Ğ¸Ñ",
        "url": f"{BASE_URL}/product-category/ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ-Ğ¸Ğ·Ğ´ĞµĞ»Ğ¸Ñ/",
    },
    {
        "name": "Ğ—Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²",
        "url": f"{BASE_URL}/product-category/Ğ·Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚Ğ¸-Ğ´Ğ»Ñ-Ñ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²/",
    },
    {
        "name": "Ğ—Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
        "url": f"{BASE_URL}/product-category/Ğ·Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚Ğ¸-Ğ´Ğ»Ñ-Ğ½Ğ°Ğ²ĞµÑĞ½Ğ¾Ğ³Ğ¾-Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ/",
    },
    {
        "name": "Ğ—Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ·ĞµĞ»ĞµĞ¹",
        "url": f"{BASE_URL}/product-category/Ğ·Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚Ğ¸-Ğ´Ğ»Ñ-Ğ´Ğ¸Ğ·ĞµĞ»ĞµĞ¹/",
    },
    {
        "name": "ĞšĞ¾Ğ»Ñ‘ÑĞ°, ÑˆĞ¸Ğ½Ñ‹, Ğ³Ñ€ÑƒĞ·Ğ°",
        "url": f"{BASE_URL}/product-category/ĞºĞ¾Ğ»Ñ‘ÑĞ°-ÑˆĞ¸Ğ½Ñ‹-Ğ³Ñ€ÑƒĞ·Ğ°/",
    },
    {"name": "ĞŸÑ€Ğ¾Ñ‡Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚Ğ¸", "url": f"{BASE_URL}/product-category/Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ğµ-Ğ·Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚Ğ¸/"},
    {
        "name": "Ğ¡Ñ‚Ğ°Ñ€Ñ‚ĞµÑ€Ñ‹, Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹",
        "url": f"{BASE_URL}/product-category/ÑÑ‚Ğ°Ñ€Ñ‚ĞµÑ€Ñ‹-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñ‹/",
    },
    {"name": "Ğ¡Ğ¸Ğ´ĞµĞ½ÑŒÑ (ĞºÑ€ĞµÑĞ»Ğ°)", "url": f"{BASE_URL}/product-category/ÑĞ¸Ğ´ĞµĞ½ÑŒÑ-ĞºÑ€ĞµÑĞ»Ğ°/"},
    {"name": "ĞĞ¶Ğ¸Ğ´Ğ°ĞµÑ‚ÑÑ", "url": f"{BASE_URL}/product-category/Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ÑÑ/"},
]

session = requests.Session()
session.headers.update(
    {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
)


def get_subcategories(category_url):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸"""
    try:
        response = session.get(category_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        subcategories = []
        # Ğ˜Ñ‰ĞµĞ¼ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
        subcat_links = soup.select(".product-categories a, .cat-item a")

        for link in subcat_links:
            href = link.get("href")
            name = link.get_text(strip=True)
            if href and name:
                subcategories.append({"name": name, "url": href})

        return subcategories
    except Exception as e:
        print(f"  âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹: {e}")
        return []


def parse_product_page(product_url):
    """ĞŸĞ°Ñ€ÑĞ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸"""
    try:
        response = session.get(product_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ
        name_elem = soup.select_one("h1.product_title, .product-title h1")
        name = name_elem.get_text(strip=True) if name_elem else ""

        # Ğ¦ĞµĞ½Ğ°
        price_elem = soup.select_one(
            ".woocommerce-Price-amount, .price ins .amount, .price .amount"
        )
        price = price_elem.get_text(strip=True) if price_elem else ""

        # Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
        image_elem = soup.select_one(
            ".woocommerce-product-gallery__image img, .product-images img"
        )
        image_url = (
            image_elem.get("src") or image_elem.get("data-src") if image_elem else ""
        )

        return {
            "name": name,
            "price": price,
            "image_url": image_url,
            "link": product_url,
        }
    except Exception as e:
        return None


def get_products_from_page(page_url, category_name):
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹ ÑĞ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³Ğ°"""
    try:
        response = session.get(page_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        products = []

        # Ğ˜Ñ‰ĞµĞ¼ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹
        product_items = soup.select(".product, .type-product, .product-grid-item")

        for item in product_items:
            # Ğ¡ÑÑ‹Ğ»ĞºĞ° Ğ½Ğ° Ñ‚Ğ¾Ğ²Ğ°Ñ€
            link_elem = item.select_one(
                "a.woocommerce-LoopProduct-link, a.product-link, h2 a, .product-title a"
            )
            if not link_elem:
                continue

            product_url = link_elem.get("href")

            # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ
            name_elem = item.select_one(
                "h2, .product-title, .woocommerce-loop-product__title"
            )
            name = name_elem.get_text(strip=True) if name_elem else ""

            # Ğ¦ĞµĞ½Ğ°
            price_elem = item.select_one(
                ".woocommerce-Price-amount, .price ins .amount, .price .amount"
            )
            price = price_elem.get_text(strip=True) if price_elem else ""

            # Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
            image_elem = item.select_one("img")
            image_url = (
                image_elem.get("src") or image_elem.get("data-src")
                if image_elem
                else ""
            )

            if name and product_url:
                products.append(
                    {
                        "name": name,
                        "price": price,
                        "image_url": image_url,
                        "link": product_url,
                        "category": category_name,
                    }
                )

        return products
    except Exception as e:
        print(f"  âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹: {e}")
        return []


def get_total_pages(category_url):
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸"""
    try:
        response = session.get(category_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        # Ğ˜Ñ‰ĞµĞ¼ Ğ¿Ğ°Ğ³Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ
        pagination = soup.select(".page-numbers a, .pagination a")
        max_page = 1

        for link in pagination:
            text = link.get_text(strip=True)
            if text.isdigit():
                max_page = max(max_page, int(text))

        return max_page
    except:
        return 1


def parse_category(category):
    """ĞŸĞ°Ñ€ÑĞ¸Ñ‚ Ğ²ÑĞµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹ Ğ¸Ğ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ĞµÑ‘ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹"""
    print(f"\n{'=' * 70}")
    print(f"ğŸ“‚ {category['name']}")
    print(f"{'=' * 70}")

    all_products = []

    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
    subcategories = get_subcategories(category["url"])

    if subcategories:
        print(f"  ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹: {len(subcategories)}")

        for subcat in subcategories:
            print(f"  ğŸ“ {subcat['name']}...")

            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
            total_pages = get_total_pages(subcat["url"])
            print(f"     Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {total_pages}")

            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ²ÑĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            for page in range(1, total_pages + 1):
                page_url = f"{subcat['url']}page/{page}/" if page > 1 else subcat["url"]
                products = get_products_from_page(page_url, category["name"])
                all_products.extend(products)

                if page % 5 == 0:
                    print(
                        f"     âœ“ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {page}/{total_pages}, Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²: {len(all_products)}"
                    )

                time.sleep(0.5)  # ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ°

            print(f"     âœ… ĞŸĞ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°. Ğ¢Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²: {len(products)}")
    else:
        # Ğ•ÑĞ»Ğ¸ Ğ½ĞµÑ‚ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹, Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼ ÑĞ°Ğ¼Ñƒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ
        print(f"  ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸...")
        total_pages = get_total_pages(category["url"])
        print(f"  Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {total_pages}")

        for page in range(1, total_pages + 1):
            page_url = f"{category['url']}page/{page}/" if page > 1 else category["url"]
            products = get_products_from_page(page_url, category["name"])
            all_products.extend(products)

            if page % 5 == 0:
                print(
                    f"  âœ“ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {page}/{total_pages}, Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²: {len(all_products)}"
                )

            time.sleep(0.5)

    print(f"\nâœ… ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°! Ğ’ÑĞµĞ³Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²: {len(all_products)}")
    return all_products


def main():
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°
    os.makedirs("parsed_data/agrodom", exist_ok=True)

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ĞŸĞĞ›ĞĞ«Ğ™ ĞŸĞĞ Ğ¡Ğ˜ĞĞ“ AGRODOM (BeautifulSoup + Requests)          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
    sys.stdout.flush()

    all_products = []

    for i, category in enumerate(CATEGORIES, 1):
        print(f"\n[{i}/{len(CATEGORIES)}] ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸...")

        try:
            products = parse_category(category)
            all_products.extend(products)

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
            with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
                json.dump(all_products, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ ĞŸÑ€Ğ¾Ğ³Ñ€ĞµÑÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½. Ğ’ÑĞµĞ³Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²: {len(all_products)}")

        except Exception as e:
            print(f"\nâŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ {category['name']}: {e}")
            continue

    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ
    unique_products = {}
    for product in all_products:
        key = product["name"].lower().strip()
        if key not in unique_products:
            unique_products[key] = product

    final_products = list(unique_products.values())

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_products, f, ensure_ascii=False, indent=2)

    print(f"\n{'=' * 70}")
    print(f"âœ… ĞŸĞĞ Ğ¡Ğ˜ĞĞ“ Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•Ğ!")
    print(f"{'=' * 70}")
    print(f"Ğ’ÑĞµĞ³Ğ¾ ÑĞ¿Ğ°Ñ€ÑĞµĞ½Ğ¾: {len(all_products)} Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²")
    print(f"Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ…: {len(final_products)} Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²")
    print(f"Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ğ²: {OUTPUT_FILE}")
    print(f"{'=' * 70}\n")


if __name__ == "__main__":
    main()
