#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ "–ó–∞–ø—á–∞—Å—Ç–∏ –¥–ª—è —Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤" —Å–æ –í–°–ï–ú–ò –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
–° –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
"""
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
import requests
from bs4 import BeautifulSoup

sys.stdout.reconfigure(line_buffering=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BASE_URL = "https://xn----7sbabpgpk4bsbesjp1f.xn--p1ai"
START_CATEGORY = "–∑–∞–ø—á–∞—Å—Ç–∏-–¥–ª—è-—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤"
OUTPUT_DIR = Path(__file__).parent.parent / "parsed_data" / "agrodom"
OUTPUT_FILE = OUTPUT_DIR / "tractors-parts-recursive.json"
LOG_FILE = OUTPUT_DIR / "tractors-parse.log"
STATE_FILE = OUTPUT_DIR / "tractors-parse-state.json"

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# HTTP —Å–µ—Å—Å–∏—è
session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
})

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
visited_urls = set()
all_products = []
seen_product_links = set()
total_categories_parsed = 0


def log(message, level="INFO"):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª –∏ –∫–æ–Ω—Å–æ–ª—å"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_message = f"[{timestamp}] [{level}] {message}"
    print(log_message)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(log_message + "\n")


def save_state():
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    state = {
        "visited_urls": list(visited_urls),
        "seen_product_links": list(seen_product_links),
        "total_products": len(all_products),
        "total_categories": total_categories_parsed,
        "timestamp": datetime.now().isoformat(),
    }
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)
    log(f"üíæ –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {len(all_products)} —Ç–æ–≤–∞—Ä–æ–≤, {total_categories_parsed} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")


def load_state():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –µ—Å–ª–∏ –µ—Å—Ç—å"""
    global visited_urls, all_products, seen_product_links, total_categories_parsed

    if STATE_FILE.exists():
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            state = json.load(f)
        visited_urls = set(state.get("visited_urls", []))
        seen_product_links = set(state.get("seen_product_links", []))
        total_categories_parsed = state.get("total_categories", 0)
        log(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: –ø—Ä–æ–ø—É—â–µ–Ω–æ {len(visited_urls)} URLs")

    if OUTPUT_FILE.exists():
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            all_products = json.load(f)
        log(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_products)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞")

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seen_product_links –∏–∑ —Ç–æ–≤–∞—Ä–æ–≤ –µ—Å–ª–∏ state —Ñ–∞–π–ª–∞ –Ω–µ –±—ã–ª–æ
        if not STATE_FILE.exists():
            seen_product_links = set(p["link"] for p in all_products)
            log(f"‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {len(seen_product_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–æ–≤–∞—Ä–æ–≤")


def save_products():
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –≤ JSON"""
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)
    log(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_products)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ {OUTPUT_FILE}")


def get_subcategories(url):
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
    try:
        log(f"üîç –ü–æ–∏—Å–∫ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π: {url.split('/')[-2]}")
        response = session.get(url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        subcats = []
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (WooCommerce —Å—Ç–∞–Ω–¥–∞—Ä—Ç)
        for link in soup.select('.product-categories a, .product-category a, .cat-item a'):
            href = link.get('href')
            if href and 'product-category' in href and href not in visited_urls:
                subcats.append(href)

        if subcats:
            log(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(subcats)}")

        return subcats
    except Exception as e:
        log(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}", "ERROR")
        return []


def get_products_from_page(url, retry=3):
    """–ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä—ã —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    for attempt in range(retry):
        try:
            response = session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            products = []
            product_cards = soup.select(".product, .type-product")

            for item in product_cards:
                try:
                    # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
                    name_elem = item.select_one(
                        "h2, .product-title, .woocommerce-loop-product__title"
                    )
                    if not name_elem:
                        continue

                    name = name_elem.get_text(strip=True)

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–∞ "–ö–∞—Ç–µ–≥–æ—Ä–∏—è (123)")
                    if "(" in name and ")" in name and name[-1] == ")":
                        continue

                    # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä
                    link_elem = item.select_one("a")
                    if not link_elem:
                        continue

                    link = link_elem.get("href", "")

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if link in seen_product_links:
                        continue

                    # –¶–µ–Ω–∞
                    price_elem = item.select_one(".price .amount, .woocommerce-Price-amount")
                    price = price_elem.get_text(strip=True) if price_elem else ""

                    # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    image_elem = item.select_one("img")
                    image_url = ""
                    if image_elem:
                        image_url = image_elem.get("src", "") or image_elem.get("data-src", "")

                    # SKU (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ)
                    sku_elem = item.select_one(".sku")
                    sku = sku_elem.get_text(strip=True) if sku_elem else ""

                    if name and link:
                        products.append({
                            "name": name,
                            "price": price,
                            "image_url": image_url,
                            "link": link,
                            "sku": sku,
                        })
                        seen_product_links.add(link)

                except Exception as e:
                    log(f"    ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞: {e}", "WARN")
                    continue

            return products

        except requests.RequestException as e:
            if attempt < retry - 1:
                log(f"  ‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retry} –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø–æ–≤—Ç–æ—Ä...", "WARN")
                time.sleep(2 * (attempt + 1))
            else:
                log(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {retry} –ø–æ–ø—ã—Ç–æ–∫: {e}", "ERROR")
                return []

    return []


def parse_category_recursive(url, depth=0, max_depth=100):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–∞—Ä—Å–∏—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –≤—Å–µ –µ—ë –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    global total_categories_parsed

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –ø–æ—Å–µ—â–∞–ª–∏ –ª–∏ —É–∂–µ
    if url in visited_urls:
        log(f"{'  ' * depth}‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ): {url.split('/')[-2]}")
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª—É–±–∏–Ω—É
    if depth > max_depth:
        log(f"{'  ' * depth}‚õî –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞", "WARN")
        return

    visited_urls.add(url)
    indent = "  " * depth
    category_name = url.split('/')[-2] if url.split('/')[-2] else url.split('/')[-3]

    log(f"{indent}{'=' * 60}")
    log(f"{indent}üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category_name} (–≥–ª—É–±–∏–Ω–∞: {depth})")
    log(f"{indent}üîó URL: {url}")

    category_products = []

    # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–∞–≥–∏–Ω–∞—Ü–∏—è)
    for page_num in range(1, 101):  # –º–∞–∫—Å–∏–º—É–º 100 —Å—Ç—Ä–∞–Ω–∏—Ü
        page_url = url if page_num == 1 else f"{url}page/{page_num}/"

        log(f"{indent}  üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}...")
        products = get_products_from_page(page_url)

        if not products:
            if page_num == 1:
                log(f"{indent}  ‚ÑπÔ∏è  –¢–æ–≤–∞—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–≤–æ–∑–º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏)")
            break

        category_products.extend(products)
        log(f"{indent}    ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")

        time.sleep(0.5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏

    if category_products:
        all_products.extend(category_products)
        log(f"{indent}  üí∞ –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(category_products)}")
        log(f"{indent}  üìä –ò–¢–û–ì–û –≤ –ë–î: {len(all_products)} —Ç–æ–≤–∞—Ä–æ–≤")

    total_categories_parsed += 1

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    save_products()
    save_state()

    # –ò—â–µ–º –∏ –ø–∞—Ä—Å–∏–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    subcategories = get_subcategories(url)

    if subcategories:
        log(f"{indent}  üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(subcategories)} –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
        for i, subcat_url in enumerate(subcategories, 1):
            log(f"{indent}  [{i}/{len(subcategories)}] –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è...")
            parse_category_recursive(subcat_url, depth + 1, max_depth)
            time.sleep(0.5)

    log(f"{indent}‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {category_name}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    log("=" * 70)
    log("üöÄ –†–ï–ö–£–†–°–ò–í–ù–´–ô –ü–ê–†–°–ò–ù–ì –ö–ê–¢–ï–ì–û–†–ò–ò '–ó–ê–ü–ß–ê–°–¢–ò –î–õ–Ø –¢–†–ê–ö–¢–û–†–û–í'")
    log("=" * 70)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    load_state()

    start_url = f"{BASE_URL}/product-category/{START_CATEGORY}/"

    log(f"üéØ –ù–∞—á–∞–ª—å–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {START_CATEGORY}")
    log(f"üîó URL: {start_url}")
    log(f"üìÅ –§–∞–π–ª –≤—ã–≤–æ–¥–∞: {OUTPUT_FILE}")
    log(f"üìù –õ–æ–≥-—Ñ–∞–π–ª: {LOG_FILE}")
    log("")

    try:
        # –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
        start_time = time.time()
        parse_category_recursive(start_url)
        elapsed = time.time() - start_time

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        log("=" * 70)
        log("üéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        log("=" * 70)
        log(f"‚úÖ –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_products)}")
        log(f"üìÇ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {total_categories_parsed}")
        log(f"üîó –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö URL: {len(visited_urls)}")
        log(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed:.2f} —Å–µ–∫ ({elapsed/60:.2f} –º–∏–Ω)")
        log(f"üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_FILE}")
        log("=" * 70)

    except KeyboardInterrupt:
        log("\n‚ö†Ô∏è  –ü–†–ï–†–í–ê–ù–û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú", "WARN")
        log("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞...")
        save_products()
        save_state()
        log("‚úÖ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è.")

    except Exception as e:
        log(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}", "ERROR")
        log("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞...")
        save_products()
        save_state()
        raise


if __name__ == "__main__":
    main()
